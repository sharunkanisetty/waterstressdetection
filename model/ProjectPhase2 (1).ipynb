{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4sxmR4dIpvq",
        "outputId": "33cf95ed-a046-4bfc-c591-5f1520394e6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2026.1.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.5)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            " Downloading dataset\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1aVVieDaek7T7ouia1VqVgrbosTP34KGr\n",
            "From (redirected): https://drive.google.com/uc?id=1aVVieDaek7T7ouia1VqVgrbosTP34KGr&confirm=t&uuid=ec28188e-b8ef-4593-9a81-5316fe06ff62\n",
            "To: /content/dataset.zip\n",
            "100%|██████████| 1.65G/1.65G [00:29<00:00, 56.2MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Extracting files...\n",
            " Extraction Complete!\n"
          ]
        }
      ],
      "source": [
        "# --- CELL 1: SETUP & DOWNLOAD ---\n",
        "!pip install opencv-python matplotlib tensorflow\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import gdown\n",
        "\n",
        "# 1. Download Dataset from Your Drive\n",
        "file_id = '1aVVieDaek7T7ouia1VqVgrbosTP34KGr'\n",
        "output_zip = 'dataset.zip'\n",
        "\n",
        "if not os.path.exists(output_zip):\n",
        "    print(\" Downloading dataset\")\n",
        "    gdown.download(f'https://drive.google.com/uc?id={file_id}', output_zip, quiet=False)\n",
        "else:\n",
        "    print(\" Dataset already downloaded.\")\n",
        "\n",
        "# 2. Extract Dataset\n",
        "extract_path = 'My_Dataset'\n",
        "if not os.path.exists(extract_path):\n",
        "    print(\" Extracting files...\")\n",
        "    with zipfile.ZipFile(output_zip, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\" Extraction Complete!\")\n",
        "else:\n",
        "    print(\" Files already extracted.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVyfMM2VI6Nw"
      },
      "outputs": [],
      "source": [
        "#  CELL 2: PROCESSING PIPELINE\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "class ThermalPipeline:\n",
        "    def __init__(self, target_size=(64, 64), seq_len=10):\n",
        "        self.target_size = target_size\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def process_single_image(self, image_path):\n",
        "\n",
        "        # 1. Acquisition\n",
        "        if not os.path.exists(image_path):\n",
        "            print(f\"Error: {image_path} not found\")\n",
        "            return None\n",
        "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # 2. Preprocessing (Resize & Normalize)\n",
        "        img = cv2.resize(img, self.target_size)\n",
        "        img = img.astype('float32') / 255.0\n",
        "        img = np.expand_dims(img, axis=-1)  # Adding channel (H, W, 1)\n",
        "\n",
        "        # 3. Representation (Repeat to make sequence)\n",
        "        # We repeat the image 'seq_len' times to simulate time\n",
        "        sequence = np.repeat(img[np.newaxis, ...], self.seq_len, axis=0)\n",
        "\n",
        "        return sequence # Shape: (Seq_Len, H, W, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Irs2fxHJADy",
        "outputId": "55c93c9d-0e2c-43cd-b5df-a470e58e5a34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Loading and processing images... this may take a moment.\n",
            " Processing folder: Non-stess\n",
            "   -> Loaded 1556 images from Non-stess\n",
            " Processing folder: Stress\n",
            "   -> Loaded 1603 images from Stress\n",
            "\n",
            " Data Ready!\n",
            "Training Data Shape: (2527, 10, 64, 64, 1) (Samples, Time, Height, Width, Channels)\n",
            "Testing Data Shape: (632, 10, 64, 64, 1)\n"
          ]
        }
      ],
      "source": [
        "# CELL 3: LOAD TRAINING DATA\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Configuration\n",
        "pipeline = ThermalPipeline(target_size=(64, 64), seq_len=10)\n",
        "data_dir = \"My_Dataset\"\n",
        "\n",
        "\n",
        "categories = [\"Non-stess\", \"Stress\"]\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "print(\" Loading and processing images... this may take a moment.\")\n",
        "\n",
        "# Walk through folders\n",
        "for label_idx, category in enumerate(categories):\n",
        "    folder_path = None\n",
        "    # Look for the folder ensuring case-insensitive matching just in case\n",
        "    for root, dirs, files in os.walk(data_dir):\n",
        "        # We look for the folder name in the directory list\n",
        "        if category in dirs:\n",
        "            folder_path = os.path.join(root, category)\n",
        "            break\n",
        "\n",
        "    if folder_path:\n",
        "        print(f\" Processing folder: {category}\")\n",
        "        file_count = 0\n",
        "        for filename in os.listdir(folder_path):\n",
        "            if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                img_path = os.path.join(folder_path, filename)\n",
        "\n",
        "                # Process the image into a sequence\n",
        "                sequence = pipeline.process_single_image(img_path)\n",
        "\n",
        "                if sequence is not None:\n",
        "                    data.append(sequence)\n",
        "                    labels.append(label_idx)\n",
        "                    file_count += 1\n",
        "        print(f\"   -> Loaded {file_count} images from {category}\")\n",
        "    else:\n",
        "        print(f\"   ERROR: Still cannot find folder '{category}'\")\n",
        "        print(f\"   Debug: Folders found in {data_dir} are: {os.listdir(data_dir)}\")\n",
        "\n",
        "# Convert to Numpy Arrays\n",
        "if len(data) > 0:\n",
        "    X = np.array(data)\n",
        "    y = np.array(labels)\n",
        "\n",
        "    # Split into Train and Test sets (80% Train, 20% Test)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(f\"\\n Data Ready!\")\n",
        "    print(f\"Training Data Shape: {X_train.shape} (Samples, Time, Height, Width, Channels)\")\n",
        "    print(f\"Testing Data Shape: {X_test.shape}\")\n",
        "else:\n",
        "    print(\"\\n CRITICAL FAILURE: No data was loaded. Please check folder names again.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
